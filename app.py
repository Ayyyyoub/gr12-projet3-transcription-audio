import streamlit as st
import whisper
import os
import tempfile
import numpy as np # N√©cessaire pour manipuler les donn√©es audio de streamlit-webrtc
from scipy.io.wavfile import write as write_wav # Pour sauvegarder l'audio enregistr√© en WAV
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase # Composants pour l'enregistrement

# --- Configuration de la Page Streamlit ---
st.set_page_config(
    page_title="Transcription Audio Pro",
    page_icon="üéôÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://github.com/openai/whisper/discussions',
        'Report a bug': "mailto:votre.email@example.com",
        'About': """
        ## Application de Transcription Audio Automatique (Projet 3)
        Cette application utilise le mod√®le **OpenAI Whisper** pour transcrire des fichiers audio.
        D√©velopp√© avec Streamlit.
        """
    }
)

# --- Fonctions Utilitaires ---
@st.cache_resource
def load_whisper_model():
    try:
        model = whisper.load_model("base")
        return model
    except Exception as e:
        st.error(f"Erreur lors du chargement du mod√®le Whisper : {e}")
        return None

# --- Classe pour le traitement audio de streamlit-webrtc ---
# Elle va accumuler les trames audio re√ßues du navigateur
class AudioRecorder(AudioProcessorBase):
    def __init__(self) -> None:
        super().__init__()
        self._frames_list = [] # Liste pour stocker les trames audio
        self.sample_rate = 16000 # Fr√©quence d'√©chantillonnage (Whisper pr√©f√®re 16kHz)
                                 # Note: le navigateur peut envoyer √† une autre fr√©quence,
                                 # une conversion pourrait √™tre n√©cessaire si ce n'est pas g√©r√© par webrtc_streamer

    def recv(self, frame):
        # frame est un objet av.AudioFrame
        # On convertit les donn√©es audio en un array numpy
        # Les donn√©es sont typiquement en int16
        # frame.layout.name donne le format (ex: 'mono', 'stereo')
        # frame.sample_rate donne la fr√©quence d'√©chantillonnage
        # st.write(f"Received frame: {frame.format.name}, {frame.sample_rate}Hz, {len(frame.planes[0])} bytes")
        self.sample_rate = frame.sample_rate # Met √† jour avec la vraie fr√©quence
        self._frames_list.append(frame.to_ndarray(format="s16", layout="flat"))
        return frame # Doit retourner la trame

    def get_recorded_data(self):
        if not self._frames_list:
            return None, None
        # Concat√©ner toutes les trames
        audio_data = np.concatenate(self._frames_list, axis=0)
        return audio_data, self.sample_rate

    def reset_frames(self):
        self._frames_list = []


# --- Chargement du Mod√®le ---
with st.spinner("Chargement du mod√®le de transcription..."):
    model = load_whisper_model()

if model is None:
    st.stop()

# --- Variables d'√©tat de session pour g√©rer l'audio enregistr√© ---
if "audio_buffer" not in st.session_state:
    st.session_state.audio_buffer = None
if "sample_rate_buffer" not in st.session_state:
    st.session_state.sample_rate_buffer = None
if "transcription_output" not in st.session_state:
    st.session_state.transcription_output = ""
if "detected_language_output" not in st.session_state:
    st.session_state.detected_language_output = ""


# --- Interface Utilisateur (UI) ---
st.title("üéôÔ∏è Projet 3 : Syst√®me de Transcription Audio Automatique")
st.markdown("""
Bienvenue ! Cette application vous permet de transcrire des fichiers audio ou d'enregistrer directement depuis votre microphone.
""")
st.markdown("---")

# --- Options d'entr√©e : Upload ou Enregistrement ---
input_method = st.radio(
    "Choisissez votre source audio :",
    ("T√©l√©verser un fichier", "Enregistrer depuis le microphone"),
    horizontal=True,
    key="input_method_radio"
)

# Colonnes pour une meilleure mise en page
col1, col2 = st.columns([0.6, 0.4]) # Zone de transcription plus large

with col2: # Colonne de droite pour les contr√¥les
    st.subheader("‚öôÔ∏è Contr√¥les")

    if input_method == "T√©l√©verser un fichier":
        st.markdown("#### 1. T√©l√©verser un fichier audio")
        uploaded_file = st.file_uploader(
            "Choisissez un fichier :",
            type=["wav", "mp3", "m4a", "ogg", "flac"],
            help="Formats support√©s : WAV, MP3, M4A, OGG, FLAC.",
            label_visibility="collapsed"
        )

        if uploaded_file is not None:
            st.write(f"Fichier s√©lectionn√© : `{uploaded_file.name}`")
            st.audio(uploaded_file, format=uploaded_file.type)
            audio_to_transcribe = uploaded_file
        else:
            st.info("Veuillez t√©l√©verser un fichier audio.")
            audio_to_transcribe = None

    elif input_method == "Enregistrer depuis le microphone":
        st.markdown("#### 1. Enregistrer l'audio")
        st.info("Cliquez sur 'START' pour commencer l'enregistrement. Le navigateur vous demandera l'autorisation d'utiliser le microphone.")

        # webrtc_ctx contiendra l'√©tat du streamer, y compris l'AudioProcessor
        webrtc_ctx = webrtc_streamer(
            key="audio-recorder",
            mode=WebRtcMode.SENDONLY, # On envoie seulement l'audio du client vers le serveur
            audio_processor_factory=AudioRecorder, # Notre classe pour traiter l'audio
            media_stream_constraints={"audio": True, "video": False}, # On ne veut que l'audio
            # sendback_audio=False # Pas besoin de renvoyer l'audio au client
        )

        if webrtc_ctx.state.playing and webrtc_ctx.audio_processor:
            st.info("üî¥ Enregistrement en cours... Cliquez sur 'STOP' pour arr√™ter.")
            # Bouton pour explicitement sauvegarder l'enregistrement (optionnel, car on peut le faire au stop)
        elif not webrtc_ctx.state.playing and webrtc_ctx.audio_processor:
            # L'enregistrement est arr√™t√©, on r√©cup√®re les donn√©es
            audio_data, sample_rate = webrtc_ctx.audio_processor.get_recorded_data()
            if audio_data is not None and sample_rate is not None:
                st.session_state.audio_buffer = audio_data
                st.session_state.sample_rate_buffer = sample_rate
                st.success("Enregistrement termin√© et sauvegard√© en m√©moire.")
                st.audio(data=audio_data, sample_rate=sample_rate, format="audio/wav")
                # Important: r√©initialiser les frames pour le prochain enregistrement
                webrtc_ctx.audio_processor.reset_frames()
            elif st.session_state.audio_buffer is not None:
                 st.info("Un enregistrement pr√©c√©dent est disponible. Cliquez sur 'Transcrire'.")
                 st.audio(data=st.session_state.audio_buffer, sample_rate=st.session_state.sample_rate_buffer, format="audio/wav")

        audio_to_transcribe = st.session_state.audio_buffer


    st.markdown("#### 2. Lancer la Transcription")
    transcribe_button_disabled = audio_to_transcribe is None
    if st.button(
        "Transcrire l'audio",
        disabled=transcribe_button_disabled,
        type="primary",
        use_container_width=True
    ):
        if audio_to_transcribe is not None:
            st.session_state.transcription_output = "" # R√©initialiser
            st.session_state.detected_language_output = "" # R√©initialiser
            progress_bar = st.progress(0, text="Initialisation...")
            temp_audio_path = None
            try:
                if hasattr(audio_to_transcribe, 'name'): # C'est un fichier upload√©
                    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(audio_to_transcribe.name)[1]) as tmp_file:
                        tmp_file.write(audio_to_transcribe.getvalue())
                        temp_audio_path = tmp_file.name
                elif isinstance(audio_to_transcribe, np.ndarray): # C'est un buffer numpy (audio enregistr√©)
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                        # √âcrire le numpy array en fichier WAV temporaire
                        # Whisper attend 16kHz, s16, mono. Si ce n'est pas le cas, ffmpeg essaiera de convertir.
                        # Pour √™tre s√ªr, on pourrait convertir ici, mais laissons ffmpeg/whisper tenter.
                        # Assurez-vous que les donn√©es sont bien en int16.
                        # Si elles sont en float, multipliez par 32767 et convertissez en int16.
                        # Ici, on suppose que AudioRecorder retourne d√©j√† du s16 (int16).
                        write_wav(tmp_file.name, st.session_state.sample_rate_buffer, audio_to_transcribe.astype(np.int16))
                        temp_audio_path = tmp_file.name
                else:
                    st.error("Type de source audio non reconnu.")
                    st.stop()

                progress_bar.progress(25, text="Pr√©paration... Lancement de la transcription Whisper...")

                options = whisper.DecodingOptions(fp16=False)
                result = model.transcribe(temp_audio_path, **vars(options))

                st.session_state.transcription_output = result["text"]
                st.session_state.detected_language_output = result["language"]

                progress_bar.progress(100, text="Termin√© !")
                st.success("Transcription effectu√©e avec succ√®s !")

            except Exception as e:
                st.error(f"‚ùå Une erreur est survenue durant la transcription :")
                st.exception(e)
                if 'progress_bar' in locals():
                     progress_bar.progress(100, text="Erreur rencontr√©e.")
            finally:
                if temp_audio_path and os.path.exists(temp_audio_path):
                    os.remove(temp_audio_path)
        else:
            st.warning("‚ö†Ô∏è Veuillez d'abord fournir un audio (uploader ou enregistrer).")


    st.sidebar.markdown("---")
    st.sidebar.markdown("### Ressources")
    st.sidebar.markdown("- [Github repo](https://github.com/Ayyyyoub/gr12-projet3-transcription-audio)")
    st.sidebar.markdown("- [Documentation Streamlit](https://docs.streamlit.io)")
    st.sidebar.markdown("- [OpenAI Whisper sur GitHub](https://github.com/openai/whisper)")
    st.sidebar.markdown("- [Streamlit-WebRTC](https://github.com/whitphx/streamlit-webrtc)")
    st.sidebar.markdown("---")
    st.sidebar.caption("Application r√©alis√©e pour le Projet 3")
    st.sidebar.caption("par Ansam EL GHIOUAN et Ayoub MAGHNOUJ.")

with col1: # Colonne de gauche pour les r√©sultats
    st.subheader("üîç R√©sultats de la Transcription")
    if st.session_state.transcription_output:
        st.text_area(
            label="Texte transcrit :",
            value=st.session_state.transcription_output,
            height=300,
            help="Vous pouvez copier ce texte."
        )
        st.info(f"üåê Langue d√©tect√©e par le mod√®le : **{st.session_state.detected_language_output.upper()}**")
    else:
        st.info("‚òùÔ∏è Les r√©sultats de la transcription appara√Ætront ici.")